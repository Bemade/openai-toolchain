{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OpenAI Toolchain","text":"<p>A minimal and intuitive library for working with OpenAI's function calling API.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udee0\ufe0f Simple function registration with <code>@tool</code> decorator</li> <li>\ud83e\udd16 Automatic tool schema generation</li> <li>\ud83d\udd04 Seamless integration with OpenAI's API</li> <li>\u26a1 Support for both sync and async operations</li> <li>\ud83d\udcda Clean and intuitive API</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from openai_toolchain import tool, OpenAIClient\n\n@tool\ndef get_weather(location: str, unit: str = \"celsius\") -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f\"The weather in {location} is 22 {unit}\"\n\n# Initialize the client with your API key\nclient = OpenAIClient(api_key=\"your-api-key\")\n\n# Use the client\nresponse = client.chat_with_tools(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Toronto?\"}]\n)\nprint(response)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install openai-toolchain\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide</li> <li>Tutorial</li> <li>API Reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"CHANGELOG/#010-2025-05-22","title":"[0.1.0] - 2025-05-22","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Initial release of OpenAI Toolchain</li> <li>Core functionality for registering and managing OpenAI function tools</li> <li><code>OpenAIClient</code> class for simplified interaction with OpenAI's API</li> <li><code>@tool</code> decorator for easy function registration</li> <li>Support for automatic tool calling and response handling</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>We welcome contributions! Here's how you can help:</p> <ol> <li>Report bugs</li> <li>Develop new features</li> <li>Improve documentation</li> <li>Submit fixes</li> </ol>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li>Clone your fork:    <pre><code>git clone https://github.com/bemade/openai-toolchain.git\ncd openai-toolchain\n</code></pre></li> <li>Set up a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></li> </ol>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>Run all tests with coverage:</p> <pre><code>pytest --cov=openai_toolchain --cov-report=term-missing\n</code></pre> <p>Run a specific test file:</p> <pre><code>pytest tests/test_module.py -v\n</code></pre>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints for all function signatures</li> <li>Include Google-style docstrings for all public functions and classes</li> <li>Keep lines under 88 characters (Black's default)</li> <li>The project uses pre-commit hooks to enforce code style:</li> <li><code>ruff</code> for linting and formatting</li> <li><code>black</code> for code formatting</li> <li><code>mypy</code> for static type checking</li> <li><code>prettier</code> for Markdown formatting</li> </ul> <p>To run all linters manually:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>We use MkDocs with Material for documentation. To build the docs locally:</p> <pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Build and serve the docs\nmkdocs serve\n</code></pre> <p>Then open <code>http://127.0.0.1:8000</code> in your browser.</p>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a feature branch from <code>main</code></li> <li>Make your changes</li> <li>Add/update tests</li> <li>Update documentation if needed</li> <li>Run tests and linters</li> <li>Submit a pull request with a clear description of changes</li> </ol>"},{"location":"CONTRIBUTING/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>This project uses pre-commit to ensure code quality. The pre-commit hooks will run automatically on each commit. To manually run the hooks:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>To skip the pre-commit checks (not recommended):</p> <pre><code>git commit --no-verify -m \"Your commit message\"\n</code></pre>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to the Contributor Covenant Code of Conduct.</p>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting issues, please include:</p> <ul> <li>A clear description of the issue</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Python version and OS</li> <li>Any relevant error messages</li> <li>If possible, include a minimal reproducible example</li> </ul>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>An OpenAI API key</li> </ul>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"<pre><code>pip install openai-toolchain\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/bemade/openai-toolchain.git\ncd openai-toolchain\n</code></pre> <ol> <li>Create and activate a virtual environment:</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> <ol> <li>Install with development dependencies:    <pre><code>pip install -r requirements-dev.txt\n</code></pre></li> </ol>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import openai_toolchain\nprint(openai_toolchain.__version__)\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>Set your OpenAI API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY='your-api-key-here'\n</code></pre> <p>Or pass it when initializing the client:</p> <pre><code>from openai_toolchain import OpenAIClient\n\nclient = OpenAIClient(api_key=\"your-api-key-here\")\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial - Learn how to use OpenAI Toolchain</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial will guide you through using OpenAI Toolchain to create and manage OpenAI function tools.</p>"},{"location":"tutorial/#basic-usage","title":"Basic Usage","text":""},{"location":"tutorial/#1-import-and-setup","title":"1. Import and Setup","text":"<pre><code>from openai_toolchain import tool, OpenAIClient\n\n# Initialize the client\nclient = OpenAIClient(api_key=\"your-api-key\")\n</code></pre>"},{"location":"tutorial/#2-create-a-tool","title":"2. Create a Tool","text":"<pre><code>@tool\ndef get_weather(location: str, unit: str = \"celsius\") -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city to get the weather for\n        unit: The unit of temperature (celsius or fahrenheit)\n\n    Returns:\n        str: The current weather information\n    \"\"\"\n    # In a real app, you would call a weather API here\n    return f\"The weather in {location} is 22 {unit}\"\n</code></pre>"},{"location":"tutorial/#3-use-the-tool","title":"3. Use the Tool","text":"<pre><code>response = client.chat_with_tools(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Toronto?\"}]\n)\nprint(response)\n</code></pre>"},{"location":"tutorial/#advanced-features","title":"Advanced Features","text":""},{"location":"tutorial/#async-support","title":"Async Support","text":"<pre><code>import asyncio\nfrom openai_toolchain import AsyncOpenAIClient\n\nasync def main():\n    client = AsyncOpenAIClient(api_key=\"your-api-key\")\n    response = await client.chat_with_tools(\n        messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}]\n    )\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/#custom-tool-naming","title":"Custom Tool Naming","text":"<pre><code>@tool(\"get_current_time\")\ndef get_time(timezone: str = \"UTC\") -&gt; str:\n    \"\"\"Get the current time in the specified timezone.\"\"\"\n    from datetime import datetime\n    import pytz\n    tz = pytz.timezone(timezone)\n    return datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n</code></pre>"},{"location":"tutorial/#tool-dependencies","title":"Tool Dependencies","text":"<pre><code>@tool\ndef get_weather_with_deps(location: str) -&gt; str:\n    \"\"\"Get weather with dependencies.\"\"\"\n    # You can use other tools or dependencies\n    from some_weather_lib import get_weather as fetch_weather\n    return fetch_weather(location)\n</code></pre>"},{"location":"tutorial/#best-practices","title":"Best Practices","text":"<ol> <li>Document Your Tools: Always include docstrings with clear parameter and    return type information.</li> <li>Handle Errors: Implement proper error handling in your tool functions.</li> <li>Type Hints: Use Python type hints for better AI tool calling, IDE support    and documentation.</li> <li>Testing: Write tests for your tools to ensure they work as expected.</li> </ol>"},{"location":"tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference for detailed documentation</li> <li>Check out more examples</li> <li>Learn how to contribute to the project</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Features","text":""},{"location":"examples/advanced/#custom-tool-naming","title":"Custom Tool Naming","text":"<p>You can specify a custom name for your tool:</p> <pre><code>from openai_toolchain import tool\n\n@tool(\"get_current_time\")\ndef get_time(timezone: str = \"UTC\") -&gt; str:\n    \"\"\"Get the current time in the specified timezone.\"\"\"\n    from datetime import datetime\n    import pytz\n    tz = pytz.timezone(timezone)\n    return datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n</code></pre>"},{"location":"examples/advanced/#tool-dependencies","title":"Tool Dependencies","text":"<p>Tools can depend on other tools or external libraries:</p> <pre><code>from openai_toolchain import tool\n\n@tool\ndef analyze_weather(location: str) -&gt; dict:\n    \"\"\"Analyze weather patterns for a location.\"\"\"\n    # This is a mock implementation\n    return {\n        \"location\": location,\n        \"temperature\": 22,\n        \"conditions\": \"sunny\",\n        \"forecast\": [\"sunny\", \"partly_cloudy\", \"rain\"]\n    }\n\n@tool\ndef suggest_activity(location: str) -&gt; str:\n    \"\"\"Suggest an activity based on weather.\"\"\"\n    weather = analyze_weather(location)\n    if weather[\"conditions\"] == \"sunny\":\n        return f\"It's sunny in {location}! Perfect for a walk in the park.\"\n    elif weather[\"conditions\"] == \"rain\":\n        return f\"It's raining in {location}. How about visiting a museum?\"\n    return f\"Weather in {location} is {weather['conditions']}. Good day to stay in and code!\"\n</code></pre>"},{"location":"examples/advanced/#error-handling-in-tools","title":"Error Handling in Tools","text":"<p>Handle errors gracefully in your tools:</p> <pre><code>from openai_toolchain import tool\n\n@tool\ndef divide_numbers(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers.\"\"\"\n    try:\n        return a / b\n    except ZeroDivisionError:\n        raise ValueError(\"Cannot divide by zero\")\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {str(e)}\")\n</code></pre>"},{"location":"examples/advanced/#using-external-apis","title":"Using External APIs","text":"<p>Example of a tool that calls an external API:</p> <pre><code>import requests\nfrom openai_toolchain import tool\n\n@tool\ndef get_github_user(username: str) -&gt; dict:\n    \"\"\"Get GitHub user information.\"\"\"\n    response = requests.get(f\"https://api.github.com/users/{username}\")\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"examples/advanced/#testing-your-tools","title":"Testing Your Tools","text":"<p>Here's how you can test your tools:</p> <pre><code>import pytest\nfrom openai_toolchain import tool\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\ndef test_add_tool():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(0, 0) == 0\n</code></pre>"},{"location":"examples/async/","title":"Async Usage Example","text":"<p>This example shows how to use OpenAI Toolchain with async/await.</p> <pre><code>import asyncio\nfrom openai_toolchain import tool, AsyncOpenAIClient\n\n@tool\nasync def fetch_data(url: str) -&gt; str:\n    \"\"\"Fetch data from a URL asynchronously.\"\"\"\n    import aiohttp\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.text()\n\nasync def main():\n    # Initialize the async client\n    client = AsyncOpenAIClient(api_key=\"your-api-key\")\n\n    # Use the client\n    response = await client.chat_with_tools(\n        messages=[{\"role\": \"user\", \"content\": \"Fetch the homepage of example.com\"}]\n    )\n    print(response)\n\n# Run the async function\nasyncio.run(main())\n</code></pre>"},{"location":"examples/async/#key-points","title":"Key Points","text":"<ul> <li>Use <code>AsyncOpenAIClient</code> instead of <code>OpenAIClient</code> for async operations</li> <li>Define async tool functions with <code>async def</code></li> <li>Use <code>await</code> when calling async methods</li> <li>Run the async code using <code>asyncio.run()</code></li> </ul>"},{"location":"examples/async/#error-handling","title":"Error Handling","text":"<p>Make sure to handle potential errors in your async operations:</p> <pre><code>try:\n    response = await client.chat_with_tools(messages=messages)\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"examples/basic/","title":"Basic Usage Example","text":"<p>This example shows the basic usage of OpenAI Toolchain.</p> <pre><code>from openai_toolchain import tool, OpenAIClient\n\n@tool\ndef get_weather(location: str, unit: str = \"celsius\") -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f\"The weather in {location} is 22 {unit}\"\n\n# Initialize the client\nclient = OpenAIClient(api_key=\"your-api-key\")\n\n# Use the client\nresponse = client.chat_with_tools(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Toronto?\"}]\n)\nprint(response)\n</code></pre>"},{"location":"examples/basic/#explanation","title":"Explanation","text":"<ol> <li>We import the necessary components from the library</li> <li>We define a tool using the <code>@tool</code> decorator</li> <li>We initialize the client with our API key</li> <li>We use the client to send a message and get a response</li> </ol> <p>The tool will automatically be registered and available for use with the OpenAI API.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains the complete API reference for OpenAI Toolchain.</p>"},{"location":"reference/#core-modules","title":"Core Modules","text":""},{"location":"reference/#main-package","title":"Main Package","text":""},{"location":"reference/#openai_toolchain","title":"<code>openai_toolchain</code>","text":"<p>OpenAI Toolchain - A Python library for working with OpenAI's function calling API.</p>"},{"location":"reference/#openai_toolchain.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>Client for interacting with the OpenAI API with tool support.</p> Source code in <code>openai_toolchain/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"Client for interacting with the OpenAI API with tool support.\"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        base_url: str = \"https://api.openai.com/v1\",\n        default_model: str = \"gpt-4\",\n        **client_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the OpenAI client.\n\n        Args:\n            api_key: Your OpenAI API key\n            base_url: Base URL for the API (defaults to OpenAI's API)\n            default_model: Default model to use for completions\n            **client_kwargs: Additional arguments to pass to the OpenAI client\n        \"\"\"\n        self.client = OpenAI(api_key=api_key, base_url=base_url, **client_kwargs)\n        self.default_model = default_model\n\n    def chat(\n        self,\n        messages: Sequence[MessageDict],\n        model: Optional[str] = None,\n        tools: Optional[Sequence[Union[ToolDefinition, str]]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs: Any,\n    ) -&gt; ChatCompletion:\n        \"\"\"Send a chat completion request with optional tool support.\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content' keys\n            model: Model to use (defaults to the client's default model)\n            tools: List of tool definitions or tool names (defaults to all registered tools)\n            tool_choice: How the model should handle tool calls\n            **kwargs: Additional arguments for the completion\n\n        Returns:\n            The chat completion response\n        \"\"\"\n        model = model or self.default_model\n\n        # Get tools from the singleton registry if not provided\n        tool_schemas: List[ToolDefinition] = []\n        if tools is None:\n            tool_schemas = tool_registry.get_openai_tools()\n        elif tools and isinstance(tools[0], str):\n            all_tools = tool_registry.get_openai_tools()\n            tool_schemas = [\n                tool\n                for tool in all_tools\n                if tool.get(\"function\", {}).get(\"name\") in tools\n            ]\n        else:\n            # Cast to List[ToolDefinition] since we know the type\n            tool_schemas = list(cast(Sequence[ToolDefinition], tools))\n\n        # Convert messages to the correct format\n        openai_messages: List[ChatCompletionMessageParam] = [\n            self._convert_message(msg) for msg in messages\n        ]\n\n        # Make the API call\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=openai_messages,\n            tools=tool_schemas if tool_schemas else None,\n            tool_choice=tool_choice if tool_schemas else None,\n            **kwargs,\n        )\n\n        return response\n\n    def chat_with_tools(\n        self,\n        messages: Sequence[MessageDict],\n        tools: Optional[Sequence[str]] = None,\n        model: Optional[str] = None,\n        max_tool_calls: int = 5,\n        **kwargs: Any,\n    ) -&gt; str:\n        # Convert messages to a list for mutation\n        conversation: List[MessageDict] = list(messages)\n        \"\"\"Send a chat completion request and handle tool calls automatically.\n\n        This will automatically execute tool calls and include their results\n        in subsequent API calls until the model returns a final response.\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content' keys\n            tools: List of tool names to use (None for all registered tools)\n            model: Model to use (defaults to the client's default model)\n            max_tool_calls: Maximum number of tool call rounds to allow\n            **kwargs: Additional arguments for the completion\n\n        Returns:\n            The final assistant message content\n\n        Raises:\n            RuntimeError: If the maximum number of tool calls is exceeded\n        \"\"\"\n        # Conversation is now a list that we can mutate\n        tool_call_count = 0\n\n        # Get tools from the singleton registry\n        tool_schemas = [\n            tool\n            for tool in tool_registry.get_openai_tools()\n            if not tools or tool.get(\"function\", {}).get(\"name\") in tools\n        ]\n\n        while tool_call_count &lt; max_tool_calls:\n            # Get the next response from the model\n            response = self.chat(\n                conversation,\n                model=model or self.default_model,\n                tools=tool_schemas or None,\n                tool_choice=\"auto\" if tool_schemas else \"none\",\n                **kwargs,\n            )\n\n            message = response.choices[0].message\n\n            # If there are no tool calls, we're done\n            if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n                return message.content or \"\"\n\n            # Process tool calls\n            tool_call_count += 1\n            for tool_call in message.tool_calls:\n                function = tool_call.function\n\n                # Execute the tool\n                try:\n                    result = tool_registry.call_tool(\n                        function.name,\n                        json.loads(function.arguments),\n                    )\n                    result_str = (\n                        json.dumps(result) if not isinstance(result, str) else result\n                    )\n                except Exception as e:\n                    result_str = f\"Error: {e!s}\"\n\n                # Add the tool response to the conversation\n                conversation.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"name\": function.name,\n                        \"content\": result_str,\n                    },\n                )\n\n        raise RuntimeError(f\"Maximum number of tool calls ({max_tool_calls}) exceeded\")\n\n    def _convert_message(self, message: MessageDict) -&gt; ChatCompletionMessageParam:\n        \"\"\"Convert a message dictionary to the proper ChatCompletionMessageParam type.\"\"\"\n        role = message.get(\"role\")\n        content = message.get(\"content\", \"\")\n\n        if role == \"system\":\n            return {\"role\": \"system\", \"content\": content}\n        elif role == \"user\":\n            return {\"role\": \"user\", \"content\": content}\n        elif role == \"assistant\":\n            return {\"role\": \"assistant\", \"content\": content}\n        elif role == \"tool\":\n            return {\n                \"role\": \"tool\",\n                \"tool_call_id\": message.get(\"tool_call_id\", \"\"),\n                \"name\": message.get(\"name\", \"\"),\n                \"content\": content,\n            }\n        elif role == \"function\":\n            return {\n                \"role\": \"function\",\n                \"name\": message.get(\"name\", \"\"),\n                \"content\": content,\n            }\n        else:\n            # Default to user message if role is not recognized\n            return {\"role\": \"user\", \"content\": str(message)}\n</code></pre>"},{"location":"reference/#openai_toolchain.OpenAIClient.__init__","title":"<code>__init__(api_key, base_url='https://api.openai.com/v1', default_model='gpt-4', **client_kwargs)</code>","text":"<p>Initialize the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Your OpenAI API key</p> required <code>base_url</code> <code>str</code> <p>Base URL for the API (defaults to OpenAI's API)</p> <code>'https://api.openai.com/v1'</code> <code>default_model</code> <code>str</code> <p>Default model to use for completions</p> <code>'gpt-4'</code> <code>**client_kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI client</p> <code>{}</code> Source code in <code>openai_toolchain/client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    base_url: str = \"https://api.openai.com/v1\",\n    default_model: str = \"gpt-4\",\n    **client_kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the OpenAI client.\n\n    Args:\n        api_key: Your OpenAI API key\n        base_url: Base URL for the API (defaults to OpenAI's API)\n        default_model: Default model to use for completions\n        **client_kwargs: Additional arguments to pass to the OpenAI client\n    \"\"\"\n    self.client = OpenAI(api_key=api_key, base_url=base_url, **client_kwargs)\n    self.default_model = default_model\n</code></pre>"},{"location":"reference/#openai_toolchain.OpenAIClient.chat","title":"<code>chat(messages, model=None, tools=None, tool_choice='auto', **kwargs)</code>","text":"<p>Send a chat completion request with optional tool support.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Sequence[MessageDict]</code> <p>List of message dictionaries with 'role' and 'content' keys</p> required <code>model</code> <code>Optional[str]</code> <p>Model to use (defaults to the client's default model)</p> <code>None</code> <code>tools</code> <code>Optional[Sequence[Union[ToolDefinition, str]]]</code> <p>List of tool definitions or tool names (defaults to all registered tools)</p> <code>None</code> <code>tool_choice</code> <code>str</code> <p>How the model should handle tool calls</p> <code>'auto'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the completion</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion</code> <p>The chat completion response</p> Source code in <code>openai_toolchain/client.py</code> <pre><code>def chat(\n    self,\n    messages: Sequence[MessageDict],\n    model: Optional[str] = None,\n    tools: Optional[Sequence[Union[ToolDefinition, str]]] = None,\n    tool_choice: str = \"auto\",\n    **kwargs: Any,\n) -&gt; ChatCompletion:\n    \"\"\"Send a chat completion request with optional tool support.\n\n    Args:\n        messages: List of message dictionaries with 'role' and 'content' keys\n        model: Model to use (defaults to the client's default model)\n        tools: List of tool definitions or tool names (defaults to all registered tools)\n        tool_choice: How the model should handle tool calls\n        **kwargs: Additional arguments for the completion\n\n    Returns:\n        The chat completion response\n    \"\"\"\n    model = model or self.default_model\n\n    # Get tools from the singleton registry if not provided\n    tool_schemas: List[ToolDefinition] = []\n    if tools is None:\n        tool_schemas = tool_registry.get_openai_tools()\n    elif tools and isinstance(tools[0], str):\n        all_tools = tool_registry.get_openai_tools()\n        tool_schemas = [\n            tool\n            for tool in all_tools\n            if tool.get(\"function\", {}).get(\"name\") in tools\n        ]\n    else:\n        # Cast to List[ToolDefinition] since we know the type\n        tool_schemas = list(cast(Sequence[ToolDefinition], tools))\n\n    # Convert messages to the correct format\n    openai_messages: List[ChatCompletionMessageParam] = [\n        self._convert_message(msg) for msg in messages\n    ]\n\n    # Make the API call\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=openai_messages,\n        tools=tool_schemas if tool_schemas else None,\n        tool_choice=tool_choice if tool_schemas else None,\n        **kwargs,\n    )\n\n    return response\n</code></pre>"},{"location":"reference/#openai_toolchain.ToolError","title":"<code>ToolError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in tool registration or execution.</p> <p>This exception is raised when there are issues with tool registration, schema generation, or during tool execution.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>class ToolError(Exception):\n    \"\"\"Exception raised for errors in tool registration or execution.\n\n    This exception is raised when there are issues with tool registration,\n    schema generation, or during tool execution.\n    \"\"\"\n</code></pre>"},{"location":"reference/#openai_toolchain.tool","title":"<code>tool(func_or_name=None, **kwargs)</code>","text":"<p>Register a function as a tool with the global registry.</p> <p>This decorator can be used with or without arguments to register a function as a tool with the global tool registry.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @tool\n... def my_function():\n...     pass\n</code></pre> <pre><code>&gt;&gt;&gt; @tool(\"custom_name\")\n... def another_function():\n...     pass\n</code></pre> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def tool(\n    func_or_name: Optional[Union[Callable[..., Any], str]] = None, **kwargs: Any\n) -&gt; Union[Callable[[Callable[..., Any]], Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"Register a function as a tool with the global registry.\n\n    This decorator can be used with or without arguments to register a function\n    as a tool with the global tool registry.\n\n    Examples:\n        &gt;&gt;&gt; @tool\n        ... def my_function():\n        ...     pass\n\n        &gt;&gt;&gt; @tool(\"custom_name\")\n        ... def another_function():\n        ...     pass\n    \"\"\"\n    \"\"\"Decorator to register a function as a tool.\n\n    This decorator provides a convenient way to register functions as tools with\n    the global tool registry. It can be used in several ways:\n\n    1. As a simple decorator::\n\n\n           @tool\n           def my_function():\n               pass\n\n    2. With a custom name::\n\n\n           @tool(\"custom_name\")\n           def my_function():\n               pass\n\n    3. With additional metadata::\n\n\n           @tool(name=\"custom_name\", category=\"weather\")\n           def get_weather():\n               pass\n\n    Args:\n        func_or_name: Either the function to decorate, a string name for the tool,\n            or None if using keyword arguments.\n        **kwargs: Additional metadata to include with the tool registration.\n            Common keys include 'name' for a custom tool name and 'description'\n            to override the function's docstring.\n\n    Returns:\n        Callable: The decorated function or a decorator function if called with\n            arguments.\n\n    \"\"\"\n    if func_or_name is None or isinstance(func_or_name, str):\n        # @tool or @tool(\"name\")\n        name = func_or_name\n        return lambda f: tool_registry.register(f, name=name, **kwargs)\n    else:\n        # @tool without arguments\n        return tool_registry.register(func_or_name, **kwargs)\n</code></pre>"},{"location":"reference/#submodules","title":"Submodules","text":"<ul> <li>Client - Client for interacting with OpenAI's API</li> <li>Tools - Tool registration and management</li> <li>Registry - Internal tool registry</li> <li>Types - Type definitions</li> </ul>"},{"location":"reference/#examples","title":"Examples","text":"<ul> <li>Basic Usage</li> <li>Async Usage</li> <li>Advanced Features</li> </ul>"},{"location":"reference/#additional-information","title":"Additional Information","text":"<ul> <li>Changelog</li> <li>Contributing</li> </ul>"},{"location":"reference/openai_toolchain/client/","title":"Client Module","text":""},{"location":"reference/openai_toolchain/client/#openai_toolchain.client","title":"<code>client</code>","text":"<p>OpenAI client for interacting with the API with tool support.</p>"},{"location":"reference/openai_toolchain/client/#openai_toolchain.client.ChatResponse","title":"<code>ChatResponse</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type for the response from the OpenAI API.</p> Source code in <code>openai_toolchain/client.py</code> <pre><code>class ChatResponse(TypedDict, total=False):\n    \"\"\"Type for the response from the OpenAI API.\"\"\"\n\n    choices: List[Dict[str, Any]]\n</code></pre>"},{"location":"reference/openai_toolchain/client/#openai_toolchain.client.MessageDict","title":"<code>MessageDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dictionary representing a chat message.</p> Source code in <code>openai_toolchain/client.py</code> <pre><code>class MessageDict(TypedDict, total=False):\n    \"\"\"Dictionary representing a chat message.\"\"\"\n\n    role: str\n    content: str\n    name: Optional[str]\n    tool_call_id: Optional[str]\n</code></pre>"},{"location":"reference/openai_toolchain/client/#openai_toolchain.client.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>Client for interacting with the OpenAI API with tool support.</p> Source code in <code>openai_toolchain/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"Client for interacting with the OpenAI API with tool support.\"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        base_url: str = \"https://api.openai.com/v1\",\n        default_model: str = \"gpt-4\",\n        **client_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the OpenAI client.\n\n        Args:\n            api_key: Your OpenAI API key\n            base_url: Base URL for the API (defaults to OpenAI's API)\n            default_model: Default model to use for completions\n            **client_kwargs: Additional arguments to pass to the OpenAI client\n        \"\"\"\n        self.client = OpenAI(api_key=api_key, base_url=base_url, **client_kwargs)\n        self.default_model = default_model\n\n    def chat(\n        self,\n        messages: Sequence[MessageDict],\n        model: Optional[str] = None,\n        tools: Optional[Sequence[Union[ToolDefinition, str]]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs: Any,\n    ) -&gt; ChatCompletion:\n        \"\"\"Send a chat completion request with optional tool support.\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content' keys\n            model: Model to use (defaults to the client's default model)\n            tools: List of tool definitions or tool names (defaults to all registered tools)\n            tool_choice: How the model should handle tool calls\n            **kwargs: Additional arguments for the completion\n\n        Returns:\n            The chat completion response\n        \"\"\"\n        model = model or self.default_model\n\n        # Get tools from the singleton registry if not provided\n        tool_schemas: List[ToolDefinition] = []\n        if tools is None:\n            tool_schemas = tool_registry.get_openai_tools()\n        elif tools and isinstance(tools[0], str):\n            all_tools = tool_registry.get_openai_tools()\n            tool_schemas = [\n                tool\n                for tool in all_tools\n                if tool.get(\"function\", {}).get(\"name\") in tools\n            ]\n        else:\n            # Cast to List[ToolDefinition] since we know the type\n            tool_schemas = list(cast(Sequence[ToolDefinition], tools))\n\n        # Convert messages to the correct format\n        openai_messages: List[ChatCompletionMessageParam] = [\n            self._convert_message(msg) for msg in messages\n        ]\n\n        # Make the API call\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=openai_messages,\n            tools=tool_schemas if tool_schemas else None,\n            tool_choice=tool_choice if tool_schemas else None,\n            **kwargs,\n        )\n\n        return response\n\n    def chat_with_tools(\n        self,\n        messages: Sequence[MessageDict],\n        tools: Optional[Sequence[str]] = None,\n        model: Optional[str] = None,\n        max_tool_calls: int = 5,\n        **kwargs: Any,\n    ) -&gt; str:\n        # Convert messages to a list for mutation\n        conversation: List[MessageDict] = list(messages)\n        \"\"\"Send a chat completion request and handle tool calls automatically.\n\n        This will automatically execute tool calls and include their results\n        in subsequent API calls until the model returns a final response.\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content' keys\n            tools: List of tool names to use (None for all registered tools)\n            model: Model to use (defaults to the client's default model)\n            max_tool_calls: Maximum number of tool call rounds to allow\n            **kwargs: Additional arguments for the completion\n\n        Returns:\n            The final assistant message content\n\n        Raises:\n            RuntimeError: If the maximum number of tool calls is exceeded\n        \"\"\"\n        # Conversation is now a list that we can mutate\n        tool_call_count = 0\n\n        # Get tools from the singleton registry\n        tool_schemas = [\n            tool\n            for tool in tool_registry.get_openai_tools()\n            if not tools or tool.get(\"function\", {}).get(\"name\") in tools\n        ]\n\n        while tool_call_count &lt; max_tool_calls:\n            # Get the next response from the model\n            response = self.chat(\n                conversation,\n                model=model or self.default_model,\n                tools=tool_schemas or None,\n                tool_choice=\"auto\" if tool_schemas else \"none\",\n                **kwargs,\n            )\n\n            message = response.choices[0].message\n\n            # If there are no tool calls, we're done\n            if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n                return message.content or \"\"\n\n            # Process tool calls\n            tool_call_count += 1\n            for tool_call in message.tool_calls:\n                function = tool_call.function\n\n                # Execute the tool\n                try:\n                    result = tool_registry.call_tool(\n                        function.name,\n                        json.loads(function.arguments),\n                    )\n                    result_str = (\n                        json.dumps(result) if not isinstance(result, str) else result\n                    )\n                except Exception as e:\n                    result_str = f\"Error: {e!s}\"\n\n                # Add the tool response to the conversation\n                conversation.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"name\": function.name,\n                        \"content\": result_str,\n                    },\n                )\n\n        raise RuntimeError(f\"Maximum number of tool calls ({max_tool_calls}) exceeded\")\n\n    def _convert_message(self, message: MessageDict) -&gt; ChatCompletionMessageParam:\n        \"\"\"Convert a message dictionary to the proper ChatCompletionMessageParam type.\"\"\"\n        role = message.get(\"role\")\n        content = message.get(\"content\", \"\")\n\n        if role == \"system\":\n            return {\"role\": \"system\", \"content\": content}\n        elif role == \"user\":\n            return {\"role\": \"user\", \"content\": content}\n        elif role == \"assistant\":\n            return {\"role\": \"assistant\", \"content\": content}\n        elif role == \"tool\":\n            return {\n                \"role\": \"tool\",\n                \"tool_call_id\": message.get(\"tool_call_id\", \"\"),\n                \"name\": message.get(\"name\", \"\"),\n                \"content\": content,\n            }\n        elif role == \"function\":\n            return {\n                \"role\": \"function\",\n                \"name\": message.get(\"name\", \"\"),\n                \"content\": content,\n            }\n        else:\n            # Default to user message if role is not recognized\n            return {\"role\": \"user\", \"content\": str(message)}\n</code></pre>"},{"location":"reference/openai_toolchain/client/#openai_toolchain.client.OpenAIClient.__init__","title":"<code>__init__(api_key, base_url='https://api.openai.com/v1', default_model='gpt-4', **client_kwargs)</code>","text":"<p>Initialize the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Your OpenAI API key</p> required <code>base_url</code> <code>str</code> <p>Base URL for the API (defaults to OpenAI's API)</p> <code>'https://api.openai.com/v1'</code> <code>default_model</code> <code>str</code> <p>Default model to use for completions</p> <code>'gpt-4'</code> <code>**client_kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI client</p> <code>{}</code> Source code in <code>openai_toolchain/client.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    base_url: str = \"https://api.openai.com/v1\",\n    default_model: str = \"gpt-4\",\n    **client_kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the OpenAI client.\n\n    Args:\n        api_key: Your OpenAI API key\n        base_url: Base URL for the API (defaults to OpenAI's API)\n        default_model: Default model to use for completions\n        **client_kwargs: Additional arguments to pass to the OpenAI client\n    \"\"\"\n    self.client = OpenAI(api_key=api_key, base_url=base_url, **client_kwargs)\n    self.default_model = default_model\n</code></pre>"},{"location":"reference/openai_toolchain/client/#openai_toolchain.client.OpenAIClient.chat","title":"<code>chat(messages, model=None, tools=None, tool_choice='auto', **kwargs)</code>","text":"<p>Send a chat completion request with optional tool support.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Sequence[MessageDict]</code> <p>List of message dictionaries with 'role' and 'content' keys</p> required <code>model</code> <code>Optional[str]</code> <p>Model to use (defaults to the client's default model)</p> <code>None</code> <code>tools</code> <code>Optional[Sequence[Union[ToolDefinition, str]]]</code> <p>List of tool definitions or tool names (defaults to all registered tools)</p> <code>None</code> <code>tool_choice</code> <code>str</code> <p>How the model should handle tool calls</p> <code>'auto'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the completion</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion</code> <p>The chat completion response</p> Source code in <code>openai_toolchain/client.py</code> <pre><code>def chat(\n    self,\n    messages: Sequence[MessageDict],\n    model: Optional[str] = None,\n    tools: Optional[Sequence[Union[ToolDefinition, str]]] = None,\n    tool_choice: str = \"auto\",\n    **kwargs: Any,\n) -&gt; ChatCompletion:\n    \"\"\"Send a chat completion request with optional tool support.\n\n    Args:\n        messages: List of message dictionaries with 'role' and 'content' keys\n        model: Model to use (defaults to the client's default model)\n        tools: List of tool definitions or tool names (defaults to all registered tools)\n        tool_choice: How the model should handle tool calls\n        **kwargs: Additional arguments for the completion\n\n    Returns:\n        The chat completion response\n    \"\"\"\n    model = model or self.default_model\n\n    # Get tools from the singleton registry if not provided\n    tool_schemas: List[ToolDefinition] = []\n    if tools is None:\n        tool_schemas = tool_registry.get_openai_tools()\n    elif tools and isinstance(tools[0], str):\n        all_tools = tool_registry.get_openai_tools()\n        tool_schemas = [\n            tool\n            for tool in all_tools\n            if tool.get(\"function\", {}).get(\"name\") in tools\n        ]\n    else:\n        # Cast to List[ToolDefinition] since we know the type\n        tool_schemas = list(cast(Sequence[ToolDefinition], tools))\n\n    # Convert messages to the correct format\n    openai_messages: List[ChatCompletionMessageParam] = [\n        self._convert_message(msg) for msg in messages\n    ]\n\n    # Make the API call\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=openai_messages,\n        tools=tool_schemas if tool_schemas else None,\n        tool_choice=tool_choice if tool_schemas else None,\n        **kwargs,\n    )\n\n    return response\n</code></pre>"},{"location":"reference/openai_toolchain/registry/","title":"Registry Module","text":""},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry","title":"<code>registry</code>","text":"<p>Tool registry for managing and calling registered tools.</p>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolError","title":"<code>ToolError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for tool-related errors.</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>class ToolError(Exception):\n    \"\"\"Base exception for tool-related errors.\"\"\"\n</code></pre>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolRegistry","title":"<code>ToolRegistry</code>","text":"<p>Singleton registry for AI tools with automatic schema generation.</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"Singleton registry for AI tools with automatic schema generation.\"\"\"\n\n    _instance: Optional[\"ToolRegistry\"] = None\n    _initialized: bool = False\n\n    def __new__(cls) -&gt; \"ToolRegistry\":\n        if cls._instance is None:\n            instance = super().__new__(cls)\n            cls._instance = instance\n        return cls._instance\n\n    def __init__(self) -&gt; None:\n        if not getattr(self, \"_initialized\", False):\n            self._tools: Dict[str, Dict[str, Any]] = {}\n            self._initialized = True\n\n    def register(\n        self,\n        func: Optional[T] = None,\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[Callable[[T], T], T]:\n        \"\"\"Register a function as a tool.\n\n        Can be used as a decorator with or without arguments.\n\n        Args:\n            func: The function to register (automatically passed when used as decorator)\n            name: Optional custom name for the tool\n            description: Optional description for the tool\n            **kwargs: Additional tool metadata\n\n        Returns:\n            The decorated function or a decorator\n        \"\"\"\n\n        def decorator(f: T) -&gt; T:\n            tool_name = name or f.__name__\n            tool_description = description or (f.__doc__ or \"\").strip()\n\n            # Extract parameter information\n            sig = inspect.signature(f)\n            parameters: Dict[str, Any] = {}\n            required: List[str] = []\n            type_hints = get_type_hints(f)\n\n            for param_name, param in sig.parameters.items():\n                if param_name == \"self\":\n                    continue\n\n                param_type = type_hints.get(param_name, str)\n                param_info = self._get_parameter_info(param, param_type)\n                parameters[param_name] = param_info\n\n                if param.default == inspect.Parameter.empty:\n                    required.append(param_name)\n\n            # Create the tool schema with proper typing\n            tool_schema: Dict[str, Any] = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool_name,\n                    \"description\": tool_description,\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": parameters,\n                    },\n                },\n            }\n\n            if required:\n                # Explicitly type the parameters dictionary\n                function_params = tool_schema[\"function\"][\"parameters\"]\n                if isinstance(function_params, MutableMapping):\n                    function_params[\"required\"] = required\n\n            # Store the tool\n            self._tools[tool_name] = {\n                \"function\": f,\n                \"schema\": tool_schema,\n                \"metadata\": kwargs,\n            }\n\n            return f\n\n        if func is not None:\n            return decorator(func)\n        return decorator\n\n    def _get_parameter_info(\n        self,\n        param: inspect.Parameter,\n        param_type: Type[Any],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Get parameter information for the schema.\n\n        Args:\n            param: The parameter to get info for\n            param_type: The type of the parameter\n\n        Returns:\n            Dictionary containing parameter schema information\n        \"\"\"\n        param_info: Dict[str, Any] = {}\n\n        # Handle different parameter types\n        if param_type in (str, int, float, bool):\n            param_info[\"type\"] = param_type.__name__\n        elif param_type == list:\n            param_info.update({\"type\": \"array\", \"items\": {\"type\": \"string\"}})\n        elif param_type == dict:\n            param_info[\"type\"] = \"object\"\n        else:\n            param_info[\"type\"] = \"string\"\n\n        # Add description if available\n        if param.annotation != inspect.Parameter.empty:\n            param_info[\"description\"] = str(param.annotation)\n\n        # Add default value if available\n        if param.default != inspect.Parameter.empty:\n            param_info[\"default\"] = param.default\n\n        return param_info\n\n    def _get_type_name(self, type_: Type[Any]) -&gt; str:\n        \"\"\"Convert Python type to JSON schema type name.\n\n        Args:\n            type_: The Python type to convert\n\n        Returns:\n            String representing the JSON schema type\n        \"\"\"\n        type_map = {\n            str: \"string\",\n            int: \"integer\",\n            float: \"number\",\n            bool: \"boolean\",\n        }\n        return type_map.get(type_, \"string\")\n\n    def get_tool(self, name: str) -&gt; Optional[Callable[..., Any]]:\n        \"\"\"Get a registered tool by name.\n\n        Args:\n            name: The name of the tool\n\n        Returns:\n            The registered function or None if not found\n        \"\"\"\n        tool = self._tools.get(name)\n        if tool is None:\n            return None\n        return tool.get(\"function\")\n\n    def call_tool(self, name: str, arguments: Dict[str, Any]) -&gt; Any:\n        \"\"\"Call a registered tool by name with the given arguments.\n\n        Args:\n            name: The name of the tool to call\n            arguments: The arguments to pass to the tool\n\n        Returns:\n            The result of the tool call\n\n        Raises:\n            ToolError: If the tool is not found or an error occurs during execution\n        \"\"\"\n        tool = self._tools.get(name)\n        if not tool:\n            raise ToolError(f\"Tool '{name}' not found\")\n\n        try:\n            return tool[\"function\"](**arguments)\n        except Exception as e:\n            _logger.error(f\"Error calling tool '{name}': {e}\", exc_info=True)\n            raise ToolError(f\"Error calling tool '{name}': {e}\") from e\n\n    def get_openai_tools(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get all registered tools in OpenAI format.\n\n        Returns:\n            List[Dict[str, Any]]: A list of tool definitions in OpenAI format\n        \"\"\"\n        return [tool[\"schema\"] for tool in self._tools.values() if \"schema\" in tool]\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered tools.\"\"\"\n        self._tools.clear()\n\n    # Make the registry callable as a decorator\n    tool = register\n</code></pre>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolRegistry.call_tool","title":"<code>call_tool(name, arguments)</code>","text":"<p>Call a registered tool by name with the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool to call</p> required <code>arguments</code> <code>Dict[str, Any]</code> <p>The arguments to pass to the tool</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the tool call</p> <p>Raises:</p> Type Description <code>ToolError</code> <p>If the tool is not found or an error occurs during execution</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>def call_tool(self, name: str, arguments: Dict[str, Any]) -&gt; Any:\n    \"\"\"Call a registered tool by name with the given arguments.\n\n    Args:\n        name: The name of the tool to call\n        arguments: The arguments to pass to the tool\n\n    Returns:\n        The result of the tool call\n\n    Raises:\n        ToolError: If the tool is not found or an error occurs during execution\n    \"\"\"\n    tool = self._tools.get(name)\n    if not tool:\n        raise ToolError(f\"Tool '{name}' not found\")\n\n    try:\n        return tool[\"function\"](**arguments)\n    except Exception as e:\n        _logger.error(f\"Error calling tool '{name}': {e}\", exc_info=True)\n        raise ToolError(f\"Error calling tool '{name}': {e}\") from e\n</code></pre>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolRegistry.clear","title":"<code>clear()</code>","text":"<p>Clear all registered tools.</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered tools.\"\"\"\n    self._tools.clear()\n</code></pre>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolRegistry.get_openai_tools","title":"<code>get_openai_tools()</code>","text":"<p>Get all registered tools in OpenAI format.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of tool definitions in OpenAI format</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>def get_openai_tools(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get all registered tools in OpenAI format.\n\n    Returns:\n        List[Dict[str, Any]]: A list of tool definitions in OpenAI format\n    \"\"\"\n    return [tool[\"schema\"] for tool in self._tools.values() if \"schema\" in tool]\n</code></pre>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolRegistry.get_tool","title":"<code>get_tool(name)</code>","text":"<p>Get a registered tool by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool</p> required <p>Returns:</p> Type Description <code>Optional[Callable[..., Any]]</code> <p>The registered function or None if not found</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>def get_tool(self, name: str) -&gt; Optional[Callable[..., Any]]:\n    \"\"\"Get a registered tool by name.\n\n    Args:\n        name: The name of the tool\n\n    Returns:\n        The registered function or None if not found\n    \"\"\"\n    tool = self._tools.get(name)\n    if tool is None:\n        return None\n    return tool.get(\"function\")\n</code></pre>"},{"location":"reference/openai_toolchain/registry/#openai_toolchain.registry.ToolRegistry.register","title":"<code>register(func=None, *, name=None, description=None, **kwargs)</code>","text":"<p>Register a function as a tool.</p> <p>Can be used as a decorator with or without arguments.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Optional[T]</code> <p>The function to register (automatically passed when used as decorator)</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Optional custom name for the tool</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optional description for the tool</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional tool metadata</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Callable[[T], T], T]</code> <p>The decorated function or a decorator</p> Source code in <code>openai_toolchain/registry.py</code> <pre><code>def register(\n    self,\n    func: Optional[T] = None,\n    *,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Callable[[T], T], T]:\n    \"\"\"Register a function as a tool.\n\n    Can be used as a decorator with or without arguments.\n\n    Args:\n        func: The function to register (automatically passed when used as decorator)\n        name: Optional custom name for the tool\n        description: Optional description for the tool\n        **kwargs: Additional tool metadata\n\n    Returns:\n        The decorated function or a decorator\n    \"\"\"\n\n    def decorator(f: T) -&gt; T:\n        tool_name = name or f.__name__\n        tool_description = description or (f.__doc__ or \"\").strip()\n\n        # Extract parameter information\n        sig = inspect.signature(f)\n        parameters: Dict[str, Any] = {}\n        required: List[str] = []\n        type_hints = get_type_hints(f)\n\n        for param_name, param in sig.parameters.items():\n            if param_name == \"self\":\n                continue\n\n            param_type = type_hints.get(param_name, str)\n            param_info = self._get_parameter_info(param, param_type)\n            parameters[param_name] = param_info\n\n            if param.default == inspect.Parameter.empty:\n                required.append(param_name)\n\n        # Create the tool schema with proper typing\n        tool_schema: Dict[str, Any] = {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool_name,\n                \"description\": tool_description,\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": parameters,\n                },\n            },\n        }\n\n        if required:\n            # Explicitly type the parameters dictionary\n            function_params = tool_schema[\"function\"][\"parameters\"]\n            if isinstance(function_params, MutableMapping):\n                function_params[\"required\"] = required\n\n        # Store the tool\n        self._tools[tool_name] = {\n            \"function\": f,\n            \"schema\": tool_schema,\n            \"metadata\": kwargs,\n        }\n\n        return f\n\n    if func is not None:\n        return decorator(func)\n    return decorator\n</code></pre>"},{"location":"reference/openai_toolchain/tools/","title":"Tools Module","text":""},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools","title":"<code>tools</code>","text":"<p>Tool registration and management for OpenAI function calling.</p> <p>This module provides a registry for AI tools with automatic schema generation, validation, and execution capabilities for use with OpenAI's function calling API.</p>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolError","title":"<code>ToolError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in tool registration or execution.</p> <p>This exception is raised when there are issues with tool registration, schema generation, or during tool execution.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>class ToolError(Exception):\n    \"\"\"Exception raised for errors in tool registration or execution.\n\n    This exception is raised when there are issues with tool registration,\n    schema generation, or during tool execution.\n    \"\"\"\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry","title":"<code>ToolRegistry</code>","text":"<p>Registry for AI tools with automatic schema generation.</p> <p>This class implements a singleton pattern to maintain a global registry of tools that can be called by the OpenAI API. It handles tool registration, schema generation, and tool execution.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>class ToolRegistry:\n    \"\"\"Registry for AI tools with automatic schema generation.\n\n    This class implements a singleton pattern to maintain a global registry\n    of tools that can be called by the OpenAI API. It handles tool registration,\n    schema generation, and tool execution.\n    \"\"\"\n\n    _instance: Optional[\"ToolRegistry\"] = None\n    _tools: Dict[str, Dict[str, Any]]\n\n    def __new__(cls) -&gt; \"ToolRegistry\":\n        \"\"\"Create a new instance or return the existing singleton instance.\n\n        Returns:\n            ToolRegistry: The singleton instance of ToolRegistry.\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._tools = {}\n        return cls._instance\n\n    def register(\n        self,\n        func: Optional[Callable[..., Any]] = None,\n        *,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[Callable[[F], F], F]:\n        \"\"\"Register a function as a tool.\n\n        This method can be used as a decorator with or without arguments.\n        It registers the function with the tool registry and generates\n        the necessary OpenAPI schema for OpenAI function calling.\n\n        Examples:\n            &gt;&gt;&gt; @tool_registry.register\n            ... def my_tool(param: str) -&gt; str:\n            ...     return f\"Processed {param}\"\n            &gt;&gt;&gt; @tool_registry.register(name=\"custom_name\")\n            ... def another_tool():\n            ...     pass\n\n        Args:\n            func: The function to register (automatically passed when used as decorator).\n            name: Optional custom name for the tool. If not provided, the function's\n                name will be used.\n            **kwargs: Additional metadata to include with the tool registration.\n\n        Returns:\n            Callable: The decorated function or a decorator function if called without\n                a function argument.\n\n        Raises:\n            ToolError: If there's an issue with the tool registration.\n        \"\"\"\n\n        def decorator(f: F) -&gt; F:\n            tool_name = name or f.__name__\n            tool_description = (f.__doc__ or \"\").strip()\n\n            # Store the tool\n            self._tools[tool_name] = {\n                \"function\": f,\n                \"description\": tool_description,\n                \"parameters\": self._get_parameters_schema(f),\n                \"metadata\": kwargs,\n            }\n            return f\n\n        if func is not None:\n            return decorator(cast(F, func))\n        return decorator\n\n    def _get_parameters_schema(self, func: Callable[..., Any]) -&gt; Dict[str, Any]:\n        \"\"\"Generate OpenAPI schema for function parameters.\n\n        This method inspects the function signature and type hints to generate\n        a JSON Schema that describes the function's parameters in a format\n        compatible with OpenAI's function calling API.\n\n        Args:\n            func (Callable): The function for which to generate the parameter schema.\n\n        Returns:\n            dict: A dictionary containing the OpenAPI schema for the function's\n                parameters.\n\n        Example:\n            &gt;&gt;&gt; def example(a: int, b: str = \"default\") -&gt; None:\n            ...     pass\n            &gt;&gt;&gt; schema = tool_registry._get_parameters_schema(example)\n            &gt;&gt;&gt; print(schema)\n            {'type': 'object', 'properties': {'a': {'type': 'integer', 'description': ''}, 'b': {'type': 'string', 'description': '', 'default': 'default'}}, 'required': ['a']}  # noqa: E501\n        \"\"\"\n        params: Dict[str, Any] = {\n            \"type\": \"object\",\n            \"properties\": {},\n            \"required\": [],\n        }\n        sig = inspect.signature(func)\n        type_hints = get_type_hints(func)\n\n        for param_name, param in sig.parameters.items():\n            # Skip 'self' and 'cls' parameters\n            if param_name in (\"self\", \"cls\"):\n                continue\n\n            param_type = type_hints.get(param_name, str)\n            param_schema = self._type_to_schema(param_type)\n            param_schema[\"description\"] = \"\"\n\n            if param.default != inspect.Parameter.empty:\n                param_schema[\"default\"] = param.default\n            else:\n                required = params[\"required\"]\n                if isinstance(required, MutableSequence):\n                    required.append(param_name)\n\n            params[\"properties\"][param_name] = param_schema\n\n        return params\n\n    def _get_parameter_info(\n        self,\n        param: inspect.Parameter,\n        param_type: Type[Any],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Get parameter information for the schema.\n\n        This method extracts the parameter's type, default value, and description\n        to include in the OpenAPI schema.\n\n        Args:\n            param (inspect.Parameter): The parameter to extract information from.\n            param_type (Type): The type of the parameter.\n\n        Returns:\n            dict: A dictionary containing the parameter's information.\n        \"\"\"\n        param_info = {\"type\": self._type_to_schema(param_type)[\"type\"]}\n\n        # Add description if available\n        if param.annotation != inspect.Parameter.empty:\n            param_info[\"description\"] = str(param.annotation)\n\n        # Add default value if available\n        if param.default != inspect.Parameter.empty:\n            param_info[\"default\"] = param.default\n\n        return param_info\n\n    def _type_to_schema(self, type_: Type[Any]) -&gt; Dict[str, Any]:\n        \"\"\"Convert Python type to OpenAPI schema.\n\n        This method maps Python types to their corresponding JSON Schema types.\n        It handles basic types (str, int, float, bool) as well as generic\n        types from the typing module (List, Dict, etc.).\n\n        Args:\n            type_ (Type): The Python type to convert to a schema.\n\n        Returns:\n            dict: A dictionary representing the JSON Schema for the type.\n\n        Note:\n            For complex or custom types, the type will be converted to a string\n            representation in the schema. For more precise control over the schema,\n            consider using Pydantic models or explicitly defining the schema.\n        \"\"\"\n        if type_ is str:\n            return {\"type\": \"string\"}\n        elif type_ is int:\n            return {\"type\": \"integer\"}\n        elif type_ is float:\n            return {\"type\": \"number\"}\n        elif type_ is bool:\n            return {\"type\": \"boolean\"}\n        elif type_ is list or type_ is List:\n            return {\"type\": \"array\", \"items\": {}}\n        elif type_ is dict or type_ is Dict:\n            return {\"type\": \"object\"}\n        else:\n            # For custom types or more complex types, default to string\n            # and include the type name in the description\n            type_name = getattr(type_, \"__name__\", str(type_))\n            return {\n                \"type\": \"string\",\n                \"description\": f\"Expected type: {type_name}\",\n                \"x-python-type\": type_name,\n            }\n\n    def get_tool(self, name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get a registered tool by name.\n\n        This method retrieves a tool's metadata and function from the registry.\n\n        Args:\n            name (str): The name of the tool to retrieve.\n\n        Returns:\n            dict or None: The tool's metadata and function, or None if not found.\n        \"\"\"\n        return self._tools.get(name)\n\n    def call_tool(self, name: str, arguments: Dict[str, Any]) -&gt; Any:\n        \"\"\"Call a registered tool by name with the given arguments.\n\n        This method executes a registered tool with the provided arguments and\n        returns the result.\n\n        Args:\n            name (str): The name of the tool to call.\n            arguments (dict): A dictionary of arguments to pass to the tool.\n\n        Returns:\n            Any: The result of the tool execution.\n\n        Raises:\n            ToolError: If the tool is not found or if there's an error during execution.\n        \"\"\"\n        tool = self.get_tool(name)\n        if not tool:\n            raise ToolError(f\"Tool '{name}' not found\")\n\n        try:\n            return tool[\"function\"](**arguments)\n        except Exception as e:\n            raise ToolError(f\"Error calling tool '{name}': {e}\") from e\n\n    def get_openai_tools(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets all registered tools in OpenAI function calling format.\n\n        This method converts all registered tools to the format expected by the\n        OpenAI API for function calling. The resulting list can be passed directly\n        to the OpenAI API's `tools` parameter.\n\n        Returns:\n            list: A list of tools in OpenAI function calling format. Each tool is a\n                dictionary with 'type' and 'function' keys, where 'function'\n                contains the tool's name, description, and parameters schema.\n\n        Example:\n            &gt;&gt;&gt; @tool_registry.register\n            ... def get_weather(location: str) -&gt; str:\n            ...     return f\"Weather in {location}: Sunny\"\n            &gt;&gt;&gt; tools = tool_registry.get_openai_tools()\n            &gt;&gt;&gt; print(tools[0]['function']['name'])\n            get_weather\n        \"\"\"\n        tools = []\n        for name, tool in self._tools.items():\n            tools.append(\n                {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": name,\n                        \"description\": tool[\"description\"],\n                        \"parameters\": tool[\"parameters\"],\n                    },\n                },\n            )\n        return tools\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears all registered tools.\"\"\"\n        self._tools.clear()\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry.__new__","title":"<code>__new__()</code>","text":"<p>Create a new instance or return the existing singleton instance.</p> <p>Returns:</p> Name Type Description <code>ToolRegistry</code> <code>ToolRegistry</code> <p>The singleton instance of ToolRegistry.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def __new__(cls) -&gt; \"ToolRegistry\":\n    \"\"\"Create a new instance or return the existing singleton instance.\n\n    Returns:\n        ToolRegistry: The singleton instance of ToolRegistry.\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._tools = {}\n    return cls._instance\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry.call_tool","title":"<code>call_tool(name, arguments)</code>","text":"<p>Call a registered tool by name with the given arguments.</p> <p>This method executes a registered tool with the provided arguments and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool to call.</p> required <code>arguments</code> <code>dict</code> <p>A dictionary of arguments to pass to the tool.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the tool execution.</p> <p>Raises:</p> Type Description <code>ToolError</code> <p>If the tool is not found or if there's an error during execution.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def call_tool(self, name: str, arguments: Dict[str, Any]) -&gt; Any:\n    \"\"\"Call a registered tool by name with the given arguments.\n\n    This method executes a registered tool with the provided arguments and\n    returns the result.\n\n    Args:\n        name (str): The name of the tool to call.\n        arguments (dict): A dictionary of arguments to pass to the tool.\n\n    Returns:\n        Any: The result of the tool execution.\n\n    Raises:\n        ToolError: If the tool is not found or if there's an error during execution.\n    \"\"\"\n    tool = self.get_tool(name)\n    if not tool:\n        raise ToolError(f\"Tool '{name}' not found\")\n\n    try:\n        return tool[\"function\"](**arguments)\n    except Exception as e:\n        raise ToolError(f\"Error calling tool '{name}': {e}\") from e\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry.clear","title":"<code>clear()</code>","text":"<p>Clears all registered tools.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears all registered tools.\"\"\"\n    self._tools.clear()\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry.get_openai_tools","title":"<code>get_openai_tools()</code>","text":"<p>Gets all registered tools in OpenAI function calling format.</p> <p>This method converts all registered tools to the format expected by the OpenAI API for function calling. The resulting list can be passed directly to the OpenAI API's <code>tools</code> parameter.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Any]]</code> <p>A list of tools in OpenAI function calling format. Each tool is a dictionary with 'type' and 'function' keys, where 'function' contains the tool's name, description, and parameters schema.</p> Example <p>@tool_registry.register ... def get_weather(location: str) -&gt; str: ...     return f\"Weather in {location}: Sunny\" tools = tool_registry.get_openai_tools() print(tools[0]['function']['name']) get_weather</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def get_openai_tools(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Gets all registered tools in OpenAI function calling format.\n\n    This method converts all registered tools to the format expected by the\n    OpenAI API for function calling. The resulting list can be passed directly\n    to the OpenAI API's `tools` parameter.\n\n    Returns:\n        list: A list of tools in OpenAI function calling format. Each tool is a\n            dictionary with 'type' and 'function' keys, where 'function'\n            contains the tool's name, description, and parameters schema.\n\n    Example:\n        &gt;&gt;&gt; @tool_registry.register\n        ... def get_weather(location: str) -&gt; str:\n        ...     return f\"Weather in {location}: Sunny\"\n        &gt;&gt;&gt; tools = tool_registry.get_openai_tools()\n        &gt;&gt;&gt; print(tools[0]['function']['name'])\n        get_weather\n    \"\"\"\n    tools = []\n    for name, tool in self._tools.items():\n        tools.append(\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": name,\n                    \"description\": tool[\"description\"],\n                    \"parameters\": tool[\"parameters\"],\n                },\n            },\n        )\n    return tools\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry.get_tool","title":"<code>get_tool(name)</code>","text":"<p>Get a registered tool by name.</p> <p>This method retrieves a tool's metadata and function from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>dict or None: The tool's metadata and function, or None if not found.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def get_tool(self, name: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get a registered tool by name.\n\n    This method retrieves a tool's metadata and function from the registry.\n\n    Args:\n        name (str): The name of the tool to retrieve.\n\n    Returns:\n        dict or None: The tool's metadata and function, or None if not found.\n    \"\"\"\n    return self._tools.get(name)\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.ToolRegistry.register","title":"<code>register(func=None, *, name=None, **kwargs)</code>","text":"<p>Register a function as a tool.</p> <p>This method can be used as a decorator with or without arguments. It registers the function with the tool registry and generates the necessary OpenAPI schema for OpenAI function calling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @tool_registry.register\n... def my_tool(param: str) -&gt; str:\n...     return f\"Processed {param}\"\n&gt;&gt;&gt; @tool_registry.register(name=\"custom_name\")\n... def another_tool():\n...     pass\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Optional[Callable[..., Any]]</code> <p>The function to register (automatically passed when used as decorator).</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Optional custom name for the tool. If not provided, the function's name will be used.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata to include with the tool registration.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Union[Callable[[F], F], F]</code> <p>The decorated function or a decorator function if called without a function argument.</p> <p>Raises:</p> Type Description <code>ToolError</code> <p>If there's an issue with the tool registration.</p> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def register(\n    self,\n    func: Optional[Callable[..., Any]] = None,\n    *,\n    name: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Callable[[F], F], F]:\n    \"\"\"Register a function as a tool.\n\n    This method can be used as a decorator with or without arguments.\n    It registers the function with the tool registry and generates\n    the necessary OpenAPI schema for OpenAI function calling.\n\n    Examples:\n        &gt;&gt;&gt; @tool_registry.register\n        ... def my_tool(param: str) -&gt; str:\n        ...     return f\"Processed {param}\"\n        &gt;&gt;&gt; @tool_registry.register(name=\"custom_name\")\n        ... def another_tool():\n        ...     pass\n\n    Args:\n        func: The function to register (automatically passed when used as decorator).\n        name: Optional custom name for the tool. If not provided, the function's\n            name will be used.\n        **kwargs: Additional metadata to include with the tool registration.\n\n    Returns:\n        Callable: The decorated function or a decorator function if called without\n            a function argument.\n\n    Raises:\n        ToolError: If there's an issue with the tool registration.\n    \"\"\"\n\n    def decorator(f: F) -&gt; F:\n        tool_name = name or f.__name__\n        tool_description = (f.__doc__ or \"\").strip()\n\n        # Store the tool\n        self._tools[tool_name] = {\n            \"function\": f,\n            \"description\": tool_description,\n            \"parameters\": self._get_parameters_schema(f),\n            \"metadata\": kwargs,\n        }\n        return f\n\n    if func is not None:\n        return decorator(cast(F, func))\n    return decorator\n</code></pre>"},{"location":"reference/openai_toolchain/tools/#openai_toolchain.tools.tool","title":"<code>tool(func_or_name=None, **kwargs)</code>","text":"<p>Register a function as a tool with the global registry.</p> <p>This decorator can be used with or without arguments to register a function as a tool with the global tool registry.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @tool\n... def my_function():\n...     pass\n</code></pre> <pre><code>&gt;&gt;&gt; @tool(\"custom_name\")\n... def another_function():\n...     pass\n</code></pre> Source code in <code>openai_toolchain/tools.py</code> <pre><code>def tool(\n    func_or_name: Optional[Union[Callable[..., Any], str]] = None, **kwargs: Any\n) -&gt; Union[Callable[[Callable[..., Any]], Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"Register a function as a tool with the global registry.\n\n    This decorator can be used with or without arguments to register a function\n    as a tool with the global tool registry.\n\n    Examples:\n        &gt;&gt;&gt; @tool\n        ... def my_function():\n        ...     pass\n\n        &gt;&gt;&gt; @tool(\"custom_name\")\n        ... def another_function():\n        ...     pass\n    \"\"\"\n    \"\"\"Decorator to register a function as a tool.\n\n    This decorator provides a convenient way to register functions as tools with\n    the global tool registry. It can be used in several ways:\n\n    1. As a simple decorator::\n\n\n           @tool\n           def my_function():\n               pass\n\n    2. With a custom name::\n\n\n           @tool(\"custom_name\")\n           def my_function():\n               pass\n\n    3. With additional metadata::\n\n\n           @tool(name=\"custom_name\", category=\"weather\")\n           def get_weather():\n               pass\n\n    Args:\n        func_or_name: Either the function to decorate, a string name for the tool,\n            or None if using keyword arguments.\n        **kwargs: Additional metadata to include with the tool registration.\n            Common keys include 'name' for a custom tool name and 'description'\n            to override the function's docstring.\n\n    Returns:\n        Callable: The decorated function or a decorator function if called with\n            arguments.\n\n    \"\"\"\n    if func_or_name is None or isinstance(func_or_name, str):\n        # @tool or @tool(\"name\")\n        name = func_or_name\n        return lambda f: tool_registry.register(f, name=name, **kwargs)\n    else:\n        # @tool without arguments\n        return tool_registry.register(func_or_name, **kwargs)\n</code></pre>"},{"location":"reference/openai_toolchain/types/","title":"Types Module","text":""},{"location":"reference/openai_toolchain/types/#openai_toolchain.types","title":"<code>types</code>","text":"<p>Type definitions for the OpenAI Toolchain.</p>"},{"location":"reference/openai_toolchain/types/#openai_toolchain.types.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A tool call made by the model.</p> Source code in <code>openai_toolchain/types.py</code> <pre><code>class ToolCall(TypedDict):\n    \"\"\"A tool call made by the model.\"\"\"\n\n    id: str\n    type: Literal[\"function\"]\n    function: ToolFunction\n</code></pre>"},{"location":"reference/openai_toolchain/types/#openai_toolchain.types.ToolFunction","title":"<code>ToolFunction</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A function that can be called by the model.</p> Source code in <code>openai_toolchain/types.py</code> <pre><code>class ToolFunction(TypedDict):\n    \"\"\"A function that can be called by the model.\"\"\"\n\n    name: str\n    arguments: str\n</code></pre>"},{"location":"reference/openai_toolchain/types/#openai_toolchain.types.ToolResult","title":"<code>ToolResult</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The result of a tool call.</p> Source code in <code>openai_toolchain/types.py</code> <pre><code>class ToolResult(TypedDict):\n    \"\"\"The result of a tool call.\"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n</code></pre>"}]}